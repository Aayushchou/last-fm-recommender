{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LAST-FM: Basic ALS Recommender\n",
    "\n",
    "This notebook implements a simple ALS recommender based on the LastFM user listening dataset. It uses spark and is written in Scala. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Imports and set up \n",
    "\n",
    "Key libraries are imported, the spark session is initialised and the listening data is loaded in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.recommendation.ALS\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "import org.apache.spark.sql.types._\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.recommendation.ALS\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/24 20:00:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@3442da76\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark:SparkSession = SparkSession.builder()\n",
    "      .master(\"local[1]\")\n",
    "      .appName(\"lastfm\")\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_path: String = ../resources/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var data_path:String = \"../resources/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(user_id,StringType,true),StructField(timestamp,StringType,true),StructField(artist_id,StringType,true),StructField(artist_name,StringType,true),StructField(track_id,StringType,true),StructField(track_name,StringType,true))\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = new StructType()\n",
    "            .add(\"user_id\", StringType, true)\n",
    "            .add(\"timestamp\", StringType, true)\n",
    "            .add(\"artist_id\", StringType, true)\n",
    "            .add(\"artist_name\", StringType, true)\n",
    "            .add(\"track_id\", StringType, true)\n",
    "            .add(\"track_name\", StringType, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|    user_id|           timestamp|           artist_id|    artist_name|            track_id|          track_name|\n",
      "+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|user_000001|2009-05-04T23:08:57Z|f1b1cf71-bd35-4e9...|      Deep Dish|                null|Fuck Me Im Famous...|\n",
      "|user_000001|2009-05-04T13:54:10Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Composition 0919 ...|\n",
      "|user_000001|2009-05-04T13:52:04Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Mc2 (Live_2009_4_15)|\n",
      "|user_000001|2009-05-04T13:42:52Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Hibari (Live_2009...|\n",
      "|user_000001|2009-05-04T13:42:11Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Mc1 (Live_2009_4_15)|\n",
      "|user_000001|2009-05-04T13:38:31Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|To Stanford (Live...|\n",
      "|user_000001|2009-05-04T13:33:28Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Improvisation (Li...|\n",
      "|user_000001|2009-05-04T13:23:45Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Glacier (Live_200...|\n",
      "|user_000001|2009-05-04T13:19:22Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Parolibre (Live_2...|\n",
      "|user_000001|2009-05-04T13:13:38Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Bibo No Aozora (L...|\n",
      "|user_000001|2009-05-04T13:06:09Z|a7f7df4a-77d8-4f1...|       坂本龍一|f7c1f8f8-b935-45e...|The Last Emperor ...|\n",
      "|user_000001|2009-05-04T13:00:48Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Happyend (Live_20...|\n",
      "|user_000001|2009-05-04T12:55:34Z|a7f7df4a-77d8-4f1...|       坂本龍一|475d4e50-cebb-4cd...|Tibetan Dance (Ve...|\n",
      "|user_000001|2009-05-04T12:51:26Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Behind The Mask (...|\n",
      "|user_000001|2009-05-03T15:48:25Z|ba2f4f3b-0293-4bc...|     Underworld|dc394163-2b78-4b5...|Boy, Boy, Boy (Sw...|\n",
      "|user_000001|2009-05-03T15:37:56Z|ba2f4f3b-0293-4bc...|     Underworld|340d9a0b-9a43-409...|Crocodile (Innerv...|\n",
      "|user_000001|2009-05-03T15:14:53Z|a16e47f5-aa54-47f...|Ennio Morricone|0b04407b-f517-4e0...|Ninna Nanna In Bl...|\n",
      "|user_000001|2009-05-03T15:10:18Z|463a94f1-2713-40b...|        Minus 8|4e78efc4-e545-47a...|      Elysian Fields|\n",
      "|user_000001|2009-05-03T15:04:31Z|ad0811ea-e213-451...|      Beanfield|fb51d2c4-cc69-412...|  Planetary Deadlock|\n",
      "|user_000001|2009-05-03T14:56:25Z|309e2dfc-678e-4d0...|       Dj Linus|4277434f-e3c2-41a...|Good Morning Love...|\n",
      "+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "listener_data: org.apache.spark.sql.DataFrame = [user_id: string, timestamp: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val listener_data = spark.read.option(\"header\", false).schema(schema).option(\"sep\", \"\\t\").csv(data_path)\n",
    "listener_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------------+--------------------+--------------------+\n",
      "|    user_id|           artist_id|     artist_name|            track_id|          track_name|\n",
      "+-----------+--------------------+----------------+--------------------+--------------------+\n",
      "|user_000001|a7f7df4a-77d8-4f1...|        坂本龍一|f7c1f8f8-b935-45e...|The Last Emperor ...|\n",
      "|user_000001|a7f7df4a-77d8-4f1...|        坂本龍一|475d4e50-cebb-4cd...|Tibetan Dance (Ve...|\n",
      "|user_000001|ba2f4f3b-0293-4bc...|      Underworld|dc394163-2b78-4b5...|Boy, Boy, Boy (Sw...|\n",
      "|user_000001|ba2f4f3b-0293-4bc...|      Underworld|340d9a0b-9a43-409...|Crocodile (Innerv...|\n",
      "|user_000001|a16e47f5-aa54-47f...| Ennio Morricone|0b04407b-f517-4e0...|Ninna Nanna In Bl...|\n",
      "|user_000001|463a94f1-2713-40b...|         Minus 8|4e78efc4-e545-47a...|      Elysian Fields|\n",
      "|user_000001|ad0811ea-e213-451...|       Beanfield|fb51d2c4-cc69-412...|  Planetary Deadlock|\n",
      "|user_000001|309e2dfc-678e-4d0...|        Dj Linus|4277434f-e3c2-41a...|Good Morning Love...|\n",
      "|user_000001|6f3d4a7b-45b2-4c0...|       Alif Tree|1151b040-8022-496...|      Deadly Species|\n",
      "|user_000001|463a94f1-2713-40b...|         Minus 8|f78c95a8-9256-475...|         Cold Fusion|\n",
      "|user_000001|45bdb5be-ec03-484...|         Wei-Chi|c4fc8802-d186-4c4...|              Clouds|\n",
      "|user_000001|3c174d9d-139b-4fa...|       Marsmobil|7217acaf-0f12-4cc...|        Sovatex 2055|\n",
      "|user_000001|f945fc79-fb9e-4e3...|           Karma|b79e44f0-2a27-4f5...|         Beach Towel|\n",
      "|user_000001|4f69da4b-55a9-434...|The Young Lovers|f2a38e8c-2eb1-4f4...|How Lonely Does I...|\n",
      "|user_000001|4f69da4b-55a9-434...|The Young Lovers|d7ffc453-7c42-4fd...|   You Make Me Dizzy|\n",
      "|user_000001|3d05eb8b-1644-414...|           4Hero|41bacfc2-c594-4e0...|       The Awakening|\n",
      "|user_000001|3d05eb8b-1644-414...|           4Hero|cc7da9f6-df0e-4f8...|   Stoke Up The Fire|\n",
      "|user_000001|3d05eb8b-1644-414...|           4Hero|0024d72c-136f-49f...|Something In The Way|\n",
      "|user_000001|3d05eb8b-1644-414...|           4Hero|2f550569-8859-434...|Play With The Cha...|\n",
      "|user_000001|3d05eb8b-1644-414...|           4Hero|00b811a4-762b-492...|             Give In|\n",
      "+-----------+--------------------+----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [user_id: string, artist_id: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = listener_data.drop(\"timestamp\").na.drop()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/24 20:12:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:12:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:12:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:12:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:12:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "+-----------+--------------------+-----+\n",
      "|    user_id|            track_id|count|\n",
      "+-----------+--------------------+-----+\n",
      "|user_000001|0025055f-39c3-43e...|    2|\n",
      "|user_000001|6f71793f-5de1-499...|    7|\n",
      "|user_000001|6fa928ac-af4f-4f1...|    4|\n",
      "|user_000001|002e254d-4624-49f...|    3|\n",
      "|user_000001|708e4199-4a97-4fe...|    3|\n",
      "|user_000001|02fd11c5-7671-418...|    1|\n",
      "|user_000001|70b41574-58aa-419...|    5|\n",
      "|user_000001|051285ba-538a-4ee...|    4|\n",
      "|user_000001|764ea28c-8c5b-470...|    1|\n",
      "|user_000001|08cc9791-ac56-47d...|    6|\n",
      "|user_000001|7ab86d3d-5ff6-4d0...|    1|\n",
      "|user_000001|0b0bf1ba-1fbd-472...|    4|\n",
      "|user_000001|7d8f77b6-1fa1-40f...|    1|\n",
      "|user_000001|0be48596-37ae-403...|    3|\n",
      "|user_000001|7d9a05e1-864d-45d...|    4|\n",
      "|user_000001|10bc45ed-1e0a-45c...|    4|\n",
      "|user_000001|80590a29-6ada-49b...|    2|\n",
      "|user_000001|1203d0a0-6be4-448...|    3|\n",
      "|user_000001|8123116a-09be-4a9...|    4|\n",
      "|user_000001|14c3afc8-4dff-47a...|    1|\n",
      "+-----------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_agg: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: string, track_id: string ... 1 more field]\n",
       "df_agg_filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: string, track_id: string ... 1 more field]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_agg = df.select(\"user_id\", \"track_id\")\n",
    "            .groupBy(\"user_id\", \"track_id\")\n",
    "            .agg(count(\"*\")alias(\"count\")).orderBy(\"user_id\")\n",
    "val df_agg_filtered = df_agg.limit(20000)\n",
    "df_agg_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/24 20:13:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:13:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:13:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:13:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:13:02 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:13:09 WARN DAGScheduler: Broadcasting large task binary with size 1126.5 KiB\n",
      "+-----------+--------------------+-----+----------+-----------+\n",
      "|    user_id|            track_id|count|user_index|track_index|\n",
      "+-----------+--------------------+-----+----------+-----------+\n",
      "|user_000001|0025055f-39c3-43e...|    2|       3.0|     1207.0|\n",
      "|user_000001|6f71793f-5de1-499...|    7|       3.0|     8790.0|\n",
      "|user_000001|6fa928ac-af4f-4f1...|    4|       3.0|     8803.0|\n",
      "|user_000001|002e254d-4624-49f...|    3|       3.0|     1211.0|\n",
      "|user_000001|708e4199-4a97-4fe...|    3|       3.0|     8865.0|\n",
      "|user_000001|02fd11c5-7671-418...|    1|       3.0|     1381.0|\n",
      "|user_000001|70b41574-58aa-419...|    5|       3.0|     8878.0|\n",
      "|user_000001|051285ba-538a-4ee...|    4|       3.0|     1519.0|\n",
      "|user_000001|764ea28c-8c5b-470...|    1|       3.0|     9235.0|\n",
      "|user_000001|08cc9791-ac56-47d...|    6|       3.0|     1774.0|\n",
      "|user_000001|7ab86d3d-5ff6-4d0...|    1|       3.0|     9532.0|\n",
      "|user_000001|0b0bf1ba-1fbd-472...|    4|       3.0|     1927.0|\n",
      "|user_000001|7d8f77b6-1fa1-40f...|    1|       3.0|     9719.0|\n",
      "|user_000001|0be48596-37ae-403...|    3|       3.0|     1984.0|\n",
      "|user_000001|7d9a05e1-864d-45d...|    4|       3.0|     9728.0|\n",
      "|user_000001|10bc45ed-1e0a-45c...|    4|       3.0|     2312.0|\n",
      "|user_000001|80590a29-6ada-49b...|    2|       3.0|     9913.0|\n",
      "|user_000001|1203d0a0-6be4-448...|    3|       3.0|     2391.0|\n",
      "|user_000001|8123116a-09be-4a9...|    4|       3.0|     9969.0|\n",
      "|user_000001|14c3afc8-4dff-47a...|    1|       3.0|     2599.0|\n",
      "+-----------+--------------------+-----+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_083dd3149cbb\n",
       "track_indexer: org.apache.spark.ml.feature.StringIndexer = strIdx_4f2654566917\n",
       "df_user_indexed: org.apache.spark.sql.DataFrame = [user_id: string, track_id: string ... 2 more fields]\n",
       "df_indexed: org.apache.spark.sql.DataFrame = [user_id: string, track_id: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//revisit to make more efficient\n",
    "\n",
    "val user_indexer = new StringIndexer()\n",
    "    .setInputCol(\"user_id\")\n",
    "    .setOutputCol(\"user_index\")\n",
    "val track_indexer = new StringIndexer()\n",
    "    .setInputCol(\"track_id\")\n",
    "    .setOutputCol(\"track_index\")\n",
    "val df_user_indexed = user_indexer.fit(df_agg_filtered).transform(df_agg_filtered)\n",
    "val df_indexed = track_indexer.fit(df_user_indexed).transform(df_user_indexed)\n",
    "df_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/24 20:13:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:13:21 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:13:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:13:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:13:22 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:13:29 WARN DAGScheduler: Broadcasting large task binary with size 1126.4 KiB\n",
      "+----------+-----------+-----+\n",
      "|user_index|track_index|count|\n",
      "+----------+-----------+-----+\n",
      "|       0.0|    11040.0|    4|\n",
      "|       0.0|     4878.0|    4|\n",
      "|       0.0|     6726.0|    7|\n",
      "|       0.0|     4599.0|    5|\n",
      "|       0.0|      451.0|    1|\n",
      "|       0.0|       21.0|   11|\n",
      "|       0.0|     6840.0|    1|\n",
      "|       0.0|     4605.0|    2|\n",
      "|       0.0|     6862.0|    1|\n",
      "|       0.0|     4617.0|    1|\n",
      "|       0.0|     6965.0|    2|\n",
      "|       0.0|     4625.0|   17|\n",
      "|       0.0|     7011.0|    4|\n",
      "|       0.0|     4646.0|    1|\n",
      "|       0.0|     7100.0|    3|\n",
      "|       0.0|      331.0|    2|\n",
      "|       0.0|     7163.0|   10|\n",
      "|       0.0|     4837.0|    3|\n",
      "|       0.0|     7175.0|    3|\n",
      "|       0.0|     4838.0|    8|\n",
      "+----------+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "user_track_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_index: double, track_index: double ... 1 more field]\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val user_track_df = df_indexed.select(\"user_index\", \"track_index\", \"count\").orderBy(\"user_index\")\n",
    "user_track_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_index: double, track_index: double ... 1 more field]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_index: double, track_index: double ... 1 more field]\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = user_track_df.randomSplit(Array[Double](0.7, 0.3), 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/24 20:14:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:14:58 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:14:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:14:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/24 20:14:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n"
     ]
    }
   ],
   "source": [
    "val als = new ALS()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.01)\n",
    "  .setUserCol(\"user_index\")\n",
    "  .setItemCol(\"track_index\")\n",
    "  .setRatingCol(\"count\")\n",
    "val model = als.fit(training)\n",
    "\n",
    "val predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Generate top 10 movie recommendations for each user\n",
    "val userRecs = model.recommendForAllUsers(10)\n",
    "// Generate top 10 user recommendations for each movie\n",
    "val movieRecs = model.recommendForAllItems(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "f49041ffd7c34285ef23254d3966654103f66da5be5824172c5f986a8f982faf"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
