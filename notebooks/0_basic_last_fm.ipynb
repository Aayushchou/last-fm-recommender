{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. LAST-FM: Baseline ALS Recommender\n",
    "\n",
    "This notebook implements a simple ALS recommender based on the LastFM user listening dataset. It uses spark and is written in Scala. Minimal data cleaning/pre-processing is performed to provide a baseline model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Imports and set up \n",
    "\n",
    "Key libraries are imported, the spark session is initialised and the listening data is loaded in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.ml.Pipeline\n",
       "import org.apache.spark.ml.recommendation.ALS\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "import org.apache.spark.ml.feature.StandardScaler\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.functions.vector_to_array\n",
       "import org.apache.spark.sql.types._\n"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.ml.Pipeline\n",
    "import org.apache.spark.ml.recommendation.ALS\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.feature.StandardScaler\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.functions.vector_to_array\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/24 22:07:52 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2f8bb184\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark:SparkSession = SparkSession.builder()\n",
    "      .master(\"local[1]\")\n",
    "      .appName(\"lastfm\")\n",
    "      .getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_path: String = ../resources/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// path to last-fm dataset. Can be downloaded here: http://millionsongdataset.com/lastfm/\n",
    "var data_path:String = \"../resources/lastfm-dataset-1K/userid-timestamp-artid-artname-traid-traname.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(user_id,StringType,true),StructField(timestamp,StringType,true),StructField(artist_id,StringType,true),StructField(artist_name,StringType,true),StructField(track_id,StringType,true),StructField(track_name,StringType,true))\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// schema defined below to set column names and types. \n",
    "val schema = new StructType()\n",
    "            .add(\"user_id\", StringType, true)\n",
    "            .add(\"timestamp\", StringType, true)\n",
    "            .add(\"artist_id\", StringType, true)\n",
    "            .add(\"artist_name\", StringType, true)\n",
    "            .add(\"track_id\", StringType, true)\n",
    "            .add(\"track_name\", StringType, true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|    user_id|           timestamp|           artist_id|    artist_name|            track_id|          track_name|\n",
      "+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "|user_000001|2009-05-04T23:08:57Z|f1b1cf71-bd35-4e9...|      Deep Dish|                null|Fuck Me Im Famous...|\n",
      "|user_000001|2009-05-04T13:54:10Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Composition 0919 ...|\n",
      "|user_000001|2009-05-04T13:52:04Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Mc2 (Live_2009_4_15)|\n",
      "|user_000001|2009-05-04T13:42:52Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Hibari (Live_2009...|\n",
      "|user_000001|2009-05-04T13:42:11Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Mc1 (Live_2009_4_15)|\n",
      "|user_000001|2009-05-04T13:38:31Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|To Stanford (Live...|\n",
      "|user_000001|2009-05-04T13:33:28Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Improvisation (Li...|\n",
      "|user_000001|2009-05-04T13:23:45Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Glacier (Live_200...|\n",
      "|user_000001|2009-05-04T13:19:22Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Parolibre (Live_2...|\n",
      "|user_000001|2009-05-04T13:13:38Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Bibo No Aozora (L...|\n",
      "|user_000001|2009-05-04T13:06:09Z|a7f7df4a-77d8-4f1...|       坂本龍一|f7c1f8f8-b935-45e...|The Last Emperor ...|\n",
      "|user_000001|2009-05-04T13:00:48Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Happyend (Live_20...|\n",
      "|user_000001|2009-05-04T12:55:34Z|a7f7df4a-77d8-4f1...|       坂本龍一|475d4e50-cebb-4cd...|Tibetan Dance (Ve...|\n",
      "|user_000001|2009-05-04T12:51:26Z|a7f7df4a-77d8-4f1...|       坂本龍一|                null|Behind The Mask (...|\n",
      "|user_000001|2009-05-03T15:48:25Z|ba2f4f3b-0293-4bc...|     Underworld|dc394163-2b78-4b5...|Boy, Boy, Boy (Sw...|\n",
      "|user_000001|2009-05-03T15:37:56Z|ba2f4f3b-0293-4bc...|     Underworld|340d9a0b-9a43-409...|Crocodile (Innerv...|\n",
      "|user_000001|2009-05-03T15:14:53Z|a16e47f5-aa54-47f...|Ennio Morricone|0b04407b-f517-4e0...|Ninna Nanna In Bl...|\n",
      "|user_000001|2009-05-03T15:10:18Z|463a94f1-2713-40b...|        Minus 8|4e78efc4-e545-47a...|      Elysian Fields|\n",
      "|user_000001|2009-05-03T15:04:31Z|ad0811ea-e213-451...|      Beanfield|fb51d2c4-cc69-412...|  Planetary Deadlock|\n",
      "|user_000001|2009-05-03T14:56:25Z|309e2dfc-678e-4d0...|       Dj Linus|4277434f-e3c2-41a...|Good Morning Love...|\n",
      "+-----------+--------------------+--------------------+---------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "listener_data: org.apache.spark.sql.DataFrame = [user_id: string, timestamp: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// read in data\n",
    "val listener_data = spark.read.option(\"header\", false).schema(schema).option(\"sep\", \"\\t\").csv(data_path)\n",
    "listener_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove all NaN values as determining ratings with missing data would be problematic for a simple model. Additionally, the timestamp column is also dropped as its not directly contributing to the user-item matrix we are trying to build. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------------+--------------------+--------------------+\n",
      "|    user_id|           artist_id|     artist_name|            track_id|          track_name|\n",
      "+-----------+--------------------+----------------+--------------------+--------------------+\n",
      "|user_000001|a7f7df4a-77d8-4f1...|        坂本龍一|f7c1f8f8-b935-45e...|The Last Emperor ...|\n",
      "|user_000001|a7f7df4a-77d8-4f1...|        坂本龍一|475d4e50-cebb-4cd...|Tibetan Dance (Ve...|\n",
      "|user_000001|ba2f4f3b-0293-4bc...|      Underworld|dc394163-2b78-4b5...|Boy, Boy, Boy (Sw...|\n",
      "|user_000001|ba2f4f3b-0293-4bc...|      Underworld|340d9a0b-9a43-409...|Crocodile (Innerv...|\n",
      "|user_000001|a16e47f5-aa54-47f...| Ennio Morricone|0b04407b-f517-4e0...|Ninna Nanna In Bl...|\n",
      "|user_000001|463a94f1-2713-40b...|         Minus 8|4e78efc4-e545-47a...|      Elysian Fields|\n",
      "|user_000001|ad0811ea-e213-451...|       Beanfield|fb51d2c4-cc69-412...|  Planetary Deadlock|\n",
      "|user_000001|309e2dfc-678e-4d0...|        Dj Linus|4277434f-e3c2-41a...|Good Morning Love...|\n",
      "|user_000001|6f3d4a7b-45b2-4c0...|       Alif Tree|1151b040-8022-496...|      Deadly Species|\n",
      "|user_000001|463a94f1-2713-40b...|         Minus 8|f78c95a8-9256-475...|         Cold Fusion|\n",
      "|user_000001|45bdb5be-ec03-484...|         Wei-Chi|c4fc8802-d186-4c4...|              Clouds|\n",
      "|user_000001|3c174d9d-139b-4fa...|       Marsmobil|7217acaf-0f12-4cc...|        Sovatex 2055|\n",
      "|user_000001|f945fc79-fb9e-4e3...|           Karma|b79e44f0-2a27-4f5...|         Beach Towel|\n",
      "|user_000001|4f69da4b-55a9-434...|The Young Lovers|f2a38e8c-2eb1-4f4...|How Lonely Does I...|\n",
      "|user_000001|4f69da4b-55a9-434...|The Young Lovers|d7ffc453-7c42-4fd...|   You Make Me Dizzy|\n",
      "|user_000001|3d05eb8b-1644-414...|           4Hero|41bacfc2-c594-4e0...|       The Awakening|\n",
      "|user_000001|3d05eb8b-1644-414...|           4Hero|cc7da9f6-df0e-4f8...|   Stoke Up The Fire|\n",
      "|user_000001|3d05eb8b-1644-414...|           4Hero|0024d72c-136f-49f...|Something In The Way|\n",
      "|user_000001|3d05eb8b-1644-414...|           4Hero|2f550569-8859-434...|Play With The Cha...|\n",
      "|user_000001|3d05eb8b-1644-414...|           4Hero|00b811a4-762b-492...|             Give In|\n",
      "+-----------+--------------------+----------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [user_id: string, artist_id: string ... 3 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = listener_data.drop(\"timestamp\").na.drop()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe is then aggregated by user and track, to get the number of times a user has heard a particular track. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/25 01:15:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:15:51 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:15:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:15:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:15:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:15:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:15:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:15:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:15:52 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:15:56 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "+-----------+--------------------+-----+\n",
      "|    user_id|            track_id|count|\n",
      "+-----------+--------------------+-----+\n",
      "|user_000001|bdfbb906-495e-466...|    4|\n",
      "|user_000001|bc2c57ba-94ad-498...|    1|\n",
      "|user_000001|010fd1c1-2aed-417...|    2|\n",
      "|user_000001|002e254d-4624-49f...|    3|\n",
      "|user_000001|01ee255d-5ea2-4e4...|    1|\n",
      "|user_000001|02fd11c5-7671-418...|    1|\n",
      "|user_000001|04bd53b8-e76f-4d4...|    4|\n",
      "|user_000001|051285ba-538a-4ee...|    4|\n",
      "|user_000001|06ca9231-8575-4a5...|    2|\n",
      "|user_000001|08cc9791-ac56-47d...|    6|\n",
      "|user_000001|089ed07e-8d54-454...|    3|\n",
      "|user_000001|0b0bf1ba-1fbd-472...|    4|\n",
      "|user_000001|09d88c26-5457-4bc...|    1|\n",
      "|user_000001|0be48596-37ae-403...|    3|\n",
      "|user_000001|0acd6ec9-a02a-488...|    1|\n",
      "|user_000001|10bc45ed-1e0a-45c...|    4|\n",
      "|user_000001|0afa2c60-c501-4f0...|    2|\n",
      "|user_000001|1203d0a0-6be4-448...|    3|\n",
      "|user_000001|0b1bfcbe-3428-46e...|    1|\n",
      "|user_000001|14c3afc8-4dff-47a...|    1|\n",
      "+-----------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_agg: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: string, track_id: string ... 1 more field]\n",
       "df_agg_filtered: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: string, track_id: string ... 1 more field]\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_agg = df.select(\"user_id\", \"track_id\")\n",
    "            .groupBy(\"user_id\", \"track_id\")\n",
    "            .agg(count(\"*\")alias(\"count\")).orderBy(\"user_id\")\n",
    "val df_agg_filtered = df_agg.limit(30000)\n",
    "df_agg_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: string, track_id: string ... 1 more field]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [user_id: string, track_id: string ... 1 more field]\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(training, test) = df_agg_filtered.randomSplit(Array[Double](0.7, 0.3), 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "91: error: value useMean is not a member of org.apache.spark.ml.feature.StandardScaler",
     "output_type": "error",
     "traceback": [
      "<console>:91: error: value useMean is not a member of org.apache.spark.ml.feature.StandardScaler",
      "possible cause: maybe a semicolon is missing before `value useMean'?",
      "         .useMean(true)",
      "          ^",
      ""
     ]
    }
   ],
   "source": [
    "//revisit to make more efficient\n",
    "\n",
    "val user_indexer = new StringIndexer()\n",
    "    .setInputCol(\"user_id\")\n",
    "    .setOutputCol(\"user_index\")\n",
    "val track_indexer = new StringIndexer()\n",
    "    .setInputCol(\"track_id\")\n",
    "    .setOutputCol(\"track_index\")\n",
    "\n",
    "val va = new VectorAssembler()\n",
    "    .setInputCols(Array(\"count\"))\n",
    "    .setOutputCol(\"count_assembled\")\n",
    "\n",
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"count_assembled\")\n",
    "  .setOutputCol(\"rating\")\n",
    "\n",
    "val tr_u = user_indexer.fit(training).transform(training)\n",
    "val tr_i = track_indexer.fit(tr_u).transform(tr_u)\n",
    "val tr_s = scaler.fit(va.transform(tr_i)).transform(va.transform(tr_i))\n",
    "val tr_full = tr_s.withColumn(\"rating_as_array\", vector_to_array(tr_s(\"rating\")).getItem(0))\n",
    "val tr_final = tr_full.select(\"user_index\", \"track_index\", \"rating_as_array\").orderBy(\"user_index\")\n",
    "\n",
    "val ts_u = user_indexer.fit(test).transform(test)\n",
    "val ts_i = track_indexer.fit(ts_u).transform(ts_u)\n",
    "val ts_s = scaler.fit(va.transform(ts_i)).transform(va.transform(ts_i))\n",
    "val ts_full = ts_s.withColumn(\"rating_as_array\", vector_to_array(ts_s(\"rating\")).getItem(0))\n",
    "val ts_final = ts_full.select(\"user_index\", \"track_index\", \"rating_as_array\").orderBy(\"user_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/25 01:17:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:19 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:26 WARN DAGScheduler: Broadcasting large task binary with size 1191.6 KiB\n",
      "+----------+-----------+--------------------+\n",
      "|user_index|track_index|     rating_as_array|\n",
      "+----------+-----------+--------------------+\n",
      "|       0.0|     1144.0|  0.1615634213662931|\n",
      "|       0.0|     1215.0|0.023080488766613297|\n",
      "|       0.0|     1155.0| 0.06924146629983989|\n",
      "|       0.0|     1156.0| 0.06924146629983989|\n",
      "|       0.0|     1157.0|0.023080488766613297|\n",
      "|       0.0|     1159.0|0.023080488766613297|\n",
      "|       0.0|     1161.0| 0.34620733149919947|\n",
      "|       0.0|      124.0|  0.7616561292982388|\n",
      "|       0.0|     1165.0|0.023080488766613297|\n",
      "|       0.0|     1169.0| 0.11540244383306648|\n",
      "|       0.0|      125.0|0.046160977533226594|\n",
      "|       0.0|     1170.0|0.023080488766613297|\n",
      "|       0.0|     1171.0|0.023080488766613297|\n",
      "|       0.0|     1172.0|  0.4846902640988792|\n",
      "|       0.0|     1173.0| 0.06924146629983989|\n",
      "|       0.0|     1174.0|0.046160977533226594|\n",
      "|       0.0|      126.0| 0.46160977533226594|\n",
      "|       0.0|     1193.0| 0.06924146629983989|\n",
      "|       0.0|     1194.0| 0.06924146629983989|\n",
      "|       0.0|     1196.0|0.023080488766613297|\n",
      "+----------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tr_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/25 01:17:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:32 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:41 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:42 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:43 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:17:51 WARN DAGScheduler: Broadcasting large task binary with size 1206.8 KiB\n",
      "22/07/25 01:17:52 WARN DAGScheduler: Broadcasting large task binary with size 1208.2 KiB\n",
      "22/07/25 01:17:52 WARN DAGScheduler: Broadcasting large task binary with size 1209.7 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1211.0 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1210.0 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1211.3 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1212.5 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1212.0 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1216.7 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1215.8 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1219.0 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1217.8 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1221.0 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1219.8 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1223.0 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1221.9 KiB\n",
      "22/07/25 01:17:53 WARN DAGScheduler: Broadcasting large task binary with size 1225.0 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1223.9 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1227.1 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1225.9 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1229.1 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1227.9 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1231.1 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1230.0 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1233.1 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1232.0 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1235.1 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1234.0 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1237.2 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1236.0 KiB\n",
      "22/07/25 01:17:54 WARN DAGScheduler: Broadcasting large task binary with size 1239.2 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1238.0 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1241.2 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1240.1 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1243.2 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1242.1 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1245.2 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1244.1 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1247.3 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1246.1 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1249.3 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1248.1 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1251.3 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1250.2 KiB\n",
      "22/07/25 01:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1253.3 KiB\n",
      "22/07/25 01:17:56 WARN DAGScheduler: Broadcasting large task binary with size 1252.2 KiB\n",
      "22/07/25 01:17:56 WARN DAGScheduler: Broadcasting large task binary with size 1254.8 KiB\n",
      "22/07/25 01:17:56 WARN DAGScheduler: Broadcasting large task binary with size 1252.7 KiB\n",
      "22/07/25 01:17:56 WARN DAGScheduler: Broadcasting large task binary with size 1262.3 KiB\n",
      "22/07/25 01:17:56 WARN DAGScheduler: Broadcasting large task binary with size 1260.3 KiB\n",
      "22/07/25 01:18:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:18:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:18:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:18:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:18:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:18:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:18:01 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "22/07/25 01:18:08 WARN DAGScheduler: Broadcasting large task binary with size 1836.4 KiB\n",
      "Root-mean-square error = 1.0764759282390737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "als: org.apache.spark.ml.recommendation.ALS = als_7187857ae7a2\n",
       "model: org.apache.spark.ml.recommendation.ALSModel = ALSModel: uid=als_7187857ae7a2, rank=10\n",
       "predictions: org.apache.spark.sql.DataFrame = [user_index: double, track_index: double ... 2 more fields]\n",
       "evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = RegressionEvaluator: uid=regEval_cbae2dd1c686, metricName=rmse, throughOrigin=false\n",
       "rmse: Double = 1.0764759282390737\n"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val als = new ALS()\n",
    "  .setMaxIter(10)\n",
    "  .setRegParam(0.01)\n",
    "  .setUserCol(\"user_index\")\n",
    "  .setImplicitPrefs(true)\n",
    "  .setItemCol(\"track_index\")\n",
    "  .setRatingCol(\"rating_as_array\")\n",
    "\n",
    "val model = als.fit(tr_final)\n",
    "\n",
    "val predictions = model.transform(ts_final)\n",
    "\n",
    "val evaluator = new RegressionEvaluator()\n",
    "  .setMetricName(\"rmse\")\n",
    "  .setLabelCol(\"rating_as_array\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "\n",
    "val rmse = evaluator.evaluate(predictions)\n",
    "println(s\"Root-mean-square error = $rmse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "userRecs: org.apache.spark.sql.DataFrame = [user_index: int, recommendations: array<struct<track_index:int,rating:float>>]\n",
       "movieRecs: org.apache.spark.sql.DataFrame = [track_index: int, recommendations: array<struct<user_index:int,rating:float>>]\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val userRecs = model.recommendForAllUsers(10)\n",
    "val movieRecs = model.recommendForAllItems(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "f49041ffd7c34285ef23254d3966654103f66da5be5824172c5f986a8f982faf"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
